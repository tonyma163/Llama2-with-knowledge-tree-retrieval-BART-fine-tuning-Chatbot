{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-08T09:58:07.678694Z","iopub.status.busy":"2024-04-08T09:58:07.677881Z","iopub.status.idle":"2024-04-08T09:58:25.501546Z","shell.execute_reply":"2024-04-08T09:58:25.500594Z","shell.execute_reply.started":"2024-04-08T09:58:07.678663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Collecting peft\n","  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n","Collecting trl\n","  Downloading trl-0.8.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.3.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.2-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.8.1-py3-none-any.whl (225 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.2-py3-none-any.whl (80 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.1/80.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Installing collected packages: shtab, tyro, bitsandbytes, trl, peft\n","Successfully installed bitsandbytes-0.43.0 peft-0.10.0 shtab-1.7.1 trl-0.8.1 tyro-0.8.2\n"]}],"source":["!pip install transformers bitsandbytes accelerate datasets peft trl"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:58:25.503622Z","iopub.status.busy":"2024-04-08T09:58:25.503316Z","iopub.status.idle":"2024-04-08T09:58:27.502046Z","shell.execute_reply":"2024-04-08T09:58:27.501178Z","shell.execute_reply.started":"2024-04-08T09:58:25.503596Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0be3e950eaeb0bc2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d81501f7a561400db08e8c925162e95c","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09c6d5483f64413d8cee38671bbb1a87","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0be3e950eaeb0bc2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e929b47dcfc45159fef5c5c308eb1ba","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import Dataset, load_dataset\n","\n","dataset = load_dataset(\n","    'json',\n","    data_files={\n","        'train':\n","        '../input/nlp-project/nlp_dataset/train.txt',\n","        'validation':\n","        '../input/nlp-project/nlp_dataset/dev.txt',\n","        'test':\n","        '../input/nlp-project/nlp_dataset/test.txt',\n","    }\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:58:27.503555Z","iopub.status.busy":"2024-04-08T09:58:27.503122Z","iopub.status.idle":"2024-04-08T09:58:27.507756Z","shell.execute_reply":"2024-04-08T09:58:27.506774Z","shell.execute_reply.started":"2024-04-08T09:58:27.503524Z"},"trusted":true},"outputs":[],"source":["model_name = 'fnlp/bart-base-chinese'"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:58:27.510777Z","iopub.status.busy":"2024-04-08T09:58:27.510135Z","iopub.status.idle":"2024-04-08T09:58:28.682158Z","shell.execute_reply":"2024-04-08T09:58:28.681197Z","shell.execute_reply.started":"2024-04-08T09:58:27.510745Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aff6ecf527184ffb98e317c4b5afd43e","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b5eb00f6b794345951797e2bf9c3ce6","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/259k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aca9159d575d4d9e9c9b287040ca06a2","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aeb000bf91244ea99d3a5aa7cc574ac5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:58:28.683683Z","iopub.status.busy":"2024-04-08T09:58:28.683386Z","iopub.status.idle":"2024-04-08T09:58:28.687967Z","shell.execute_reply":"2024-04-08T09:58:28.687042Z","shell.execute_reply.started":"2024-04-08T09:58:28.683659Z"},"trusted":true},"outputs":[],"source":["# Ensure the tokenizer recognizes a pad token\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = 'right'"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:58:28.689452Z","iopub.status.busy":"2024-04-08T09:58:28.689188Z","iopub.status.idle":"2024-04-08T09:58:29.434411Z","shell.execute_reply":"2024-04-08T09:58:29.432748Z","shell.execute_reply.started":"2024-04-08T09:58:28.689424Z"},"trusted":true},"outputs":[],"source":["def create_training_examples(conversations):\n","    inputs, targets = [], []  # Initialize the lists for inputs and targets\n","    for conversation in conversations:\n","        for i in range(1, len(conversation), 2):  # Iterate over bot responses\n","            # Validate the alternating pattern: the message before a bot response should be from the user\n","            if i - 1 >= 0 and conversation[i - 1].startswith(\"user:\"):\n","                # Prepare context with up to 8 most recent turns, ensuring it ends with a user's message\n","                start_idx = max(0, i - 15)  # Adjust to capture up to 8 turns\n","                context = conversation[start_idx:i]  # Exclude the bot response itself from the context\n","\n","                # Format each turn in the context\n","                formatted_context = []\n","                for turn in context:\n","                    # Determine the prefix and remove \"user:\"/\"bot:\" from the text\n","                    if turn.startswith(\"user:\"):\n","                        prefix = \"\\nInstruction:\\n\"  # Add newline before each \"Instruction:\" for proper formatting\n","                    else:\n","                        prefix = \"\\nResponse:\\n\"\n","                    turn_text = turn.split(': ', 1)[1] if ': ' in turn else turn\n","                    formatted_context.append(f\"{prefix}{turn_text}\")\n","\n","                # Join the formatted context turns into a single input string\n","                input_text = \"\".join(formatted_context).lstrip('\\n')  # Remove leading newline\n","\n","                # Remove the \"bot:\" prefix from the target and format it\n","                target_text = conversation[i].split(': ', 1)[1] if ': ' in conversation[i] else conversation[i]\n","\n","                inputs.append(input_text)\n","                targets.append(target_text)\n","\n","    return {\"input\": inputs, \"target\": targets}\n","\n","# Apply the function to each conversation in the dataset splits\n","processed_datasets = {split: Dataset.from_dict(create_training_examples(dataset[split]['conversation'])) for split in dataset.keys()}"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:20.102588Z","iopub.status.busy":"2024-04-08T08:45:20.102242Z","iopub.status.idle":"2024-04-08T08:45:20.242693Z","shell.execute_reply":"2024-04-08T08:45:20.241705Z","shell.execute_reply.started":"2024-04-08T08:45:20.102563Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input: Instruction:\n","你好，今天是几号了？\n","###output: 你好，今天是2018年1月18日。\n"]}],"source":["print('###input:',processed_datasets['train']['input'][0])\n","print('###output:',processed_datasets['train']['target'][0])"]},{"cell_type":"code","execution_count":134,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:21.178457Z","iopub.status.busy":"2024-04-08T08:45:21.177586Z","iopub.status.idle":"2024-04-08T08:45:21.312489Z","shell.execute_reply":"2024-04-08T08:45:21.311452Z","shell.execute_reply.started":"2024-04-08T08:45:21.178419Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input: Instruction:\n","你好，今天是几号了？\n","Response:\n","你好，今天是2018年1月18日。\n","Instruction:\n","谢谢你，我都给忙忘记了。\n","###output: 哈哈，我还知道今天是周杰伦的生日呢。\n"]}],"source":["print('###input:',processed_datasets['train']['input'][1])\n","print('###output:',processed_datasets['train']['target'][1])"]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:22.111092Z","iopub.status.busy":"2024-04-08T08:45:22.110480Z","iopub.status.idle":"2024-04-08T08:45:22.243398Z","shell.execute_reply":"2024-04-08T08:45:22.242237Z","shell.execute_reply.started":"2024-04-08T08:45:22.111056Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input Instruction:\n","你好，今天是几号了？\n","Response:\n","你好，今天是2018年1月18日。\n","Instruction:\n","谢谢你，我都给忙忘记了。\n","Response:\n","哈哈，我还知道今天是周杰伦的生日呢。\n","Instruction:\n","真的吗？我最喜欢听他的歌了，他是我的偶像呢。\n","###output: 真的呢，他的歌声确实是很独特，还获得过四届新加坡金曲奖最受欢迎男歌手奖呢。\n"]}],"source":["print('###input',processed_datasets['train']['input'][2])\n","print('###output:',processed_datasets['train']['target'][2])"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:22.931944Z","iopub.status.busy":"2024-04-08T08:45:22.931281Z","iopub.status.idle":"2024-04-08T08:45:23.064478Z","shell.execute_reply":"2024-04-08T08:45:23.063421Z","shell.execute_reply.started":"2024-04-08T08:45:22.931911Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input Instruction:\n","你好，今天是几号了？\n","Response:\n","你好，今天是2018年1月18日。\n","Instruction:\n","谢谢你，我都给忙忘记了。\n","Response:\n","哈哈，我还知道今天是周杰伦的生日呢。\n","Instruction:\n","真的吗？我最喜欢听他的歌了，他是我的偶像呢。\n","Response:\n","真的呢，他的歌声确实是很独特，还获得过四届新加坡金曲奖最受欢迎男歌手奖呢。\n","Instruction:\n","高中时候很喜欢他的歌，后面只是很少关注他了。\n","###output: 那他演唱的『免费教学录像带』很赞呢，明显还是这么天马行空，里面各种可爱的小东东，推荐给你。\n"]}],"source":["print('###input',processed_datasets['train']['input'][3])\n","print('###output:',processed_datasets['train']['target'][3])"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:23.758996Z","iopub.status.busy":"2024-04-08T08:45:23.758285Z","iopub.status.idle":"2024-04-08T08:45:23.891232Z","shell.execute_reply":"2024-04-08T08:45:23.889820Z","shell.execute_reply.started":"2024-04-08T08:45:23.758961Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input Instruction:\n","你好，今天是几号了？\n","Response:\n","你好，今天是2018年1月18日。\n","Instruction:\n","谢谢你，我都给忙忘记了。\n","Response:\n","哈哈，我还知道今天是周杰伦的生日呢。\n","Instruction:\n","真的吗？我最喜欢听他的歌了，他是我的偶像呢。\n","Response:\n","真的呢，他的歌声确实是很独特，还获得过四届新加坡金曲奖最受欢迎男歌手奖呢。\n","Instruction:\n","高中时候很喜欢他的歌，后面只是很少关注他了。\n","Response:\n","那他演唱的『免费教学录像带』很赞呢，明显还是这么天马行空，里面各种可爱的小东东，推荐给你。\n","Instruction:\n","是新出来的歌么？看这个歌名不错啊。\n","###output: 很多人都喜欢听呢。\n"]}],"source":["print('###input',processed_datasets['train']['input'][4])\n","print('###output:',processed_datasets['train']['target'][4])"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:24.634747Z","iopub.status.busy":"2024-04-08T08:45:24.634360Z","iopub.status.idle":"2024-04-08T08:45:24.765219Z","shell.execute_reply":"2024-04-08T08:45:24.764127Z","shell.execute_reply.started":"2024-04-08T08:45:24.634715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input Instruction:\n","你好，今天是几号了？\n","Response:\n","你好，今天是2018年1月18日。\n","Instruction:\n","谢谢你，我都给忙忘记了。\n","Response:\n","哈哈，我还知道今天是周杰伦的生日呢。\n","Instruction:\n","真的吗？我最喜欢听他的歌了，他是我的偶像呢。\n","Response:\n","真的呢，他的歌声确实是很独特，还获得过四届新加坡金曲奖最受欢迎男歌手奖呢。\n","Instruction:\n","高中时候很喜欢他的歌，后面只是很少关注他了。\n","Response:\n","那他演唱的『免费教学录像带』很赞呢，明显还是这么天马行空，里面各种可爱的小东东，推荐给你。\n","Instruction:\n","是新出来的歌么？看这个歌名不错啊。\n","Response:\n","很多人都喜欢听呢。\n","Instruction:\n","说的我都想听一听了。\n","###output: 那你现在有时间吗？我可以为你播放。\n"]}],"source":["print('###input',processed_datasets['train']['input'][5])\n","print('###output:',processed_datasets['train']['target'][5])"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:25.605716Z","iopub.status.busy":"2024-04-08T08:45:25.604808Z","iopub.status.idle":"2024-04-08T08:45:25.738255Z","shell.execute_reply":"2024-04-08T08:45:25.737303Z","shell.execute_reply.started":"2024-04-08T08:45:25.605678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input Instruction:\n","你好，今天是几号了？\n","Response:\n","你好，今天是2018年1月18日。\n","Instruction:\n","谢谢你，我都给忙忘记了。\n","Response:\n","哈哈，我还知道今天是周杰伦的生日呢。\n","Instruction:\n","真的吗？我最喜欢听他的歌了，他是我的偶像呢。\n","Response:\n","真的呢，他的歌声确实是很独特，还获得过四届新加坡金曲奖最受欢迎男歌手奖呢。\n","Instruction:\n","高中时候很喜欢他的歌，后面只是很少关注他了。\n","Response:\n","那他演唱的『免费教学录像带』很赞呢，明显还是这么天马行空，里面各种可爱的小东东，推荐给你。\n","Instruction:\n","是新出来的歌么？看这个歌名不错啊。\n","Response:\n","很多人都喜欢听呢。\n","Instruction:\n","说的我都想听一听了。\n","Response:\n","那你现在有时间吗？我可以为你播放。\n","Instruction:\n","嗯嗯，快点播放吧，我都有点迫不及待了。\n","###output: 好的呢，已经为你播放，请你欣赏。\n"]}],"source":["print('###input',processed_datasets['train']['input'][6])\n","print('###output:',processed_datasets['train']['target'][6])"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:26.683220Z","iopub.status.busy":"2024-04-08T08:45:26.682851Z","iopub.status.idle":"2024-04-08T08:45:26.814013Z","shell.execute_reply":"2024-04-08T08:45:26.813020Z","shell.execute_reply.started":"2024-04-08T08:45:26.683194Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input Instruction:\n","你好，今天是几号了？\n","Response:\n","你好，今天是2018年1月18日。\n","Instruction:\n","谢谢你，我都给忙忘记了。\n","Response:\n","哈哈，我还知道今天是周杰伦的生日呢。\n","Instruction:\n","真的吗？我最喜欢听他的歌了，他是我的偶像呢。\n","Response:\n","真的呢，他的歌声确实是很独特，还获得过四届新加坡金曲奖最受欢迎男歌手奖呢。\n","Instruction:\n","高中时候很喜欢他的歌，后面只是很少关注他了。\n","Response:\n","那他演唱的『免费教学录像带』很赞呢，明显还是这么天马行空，里面各种可爱的小东东，推荐给你。\n","Instruction:\n","是新出来的歌么？看这个歌名不错啊。\n","Response:\n","很多人都喜欢听呢。\n","Instruction:\n","说的我都想听一听了。\n","Response:\n","那你现在有时间吗？我可以为你播放。\n","Instruction:\n","嗯嗯，快点播放吧，我都有点迫不及待了。\n","Response:\n","好的呢，已经为你播放，请你欣赏。\n","Instruction:\n","真不愧是周董的歌啊，还是熟悉的感觉。不说了，我上班去了，回来聊。\n","###output: 好的，拜拜~\n"]}],"source":["print('###input',processed_datasets['train']['input'][7])\n","print('###output:',processed_datasets['train']['target'][7])"]},{"cell_type":"code","execution_count":141,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:27.676554Z","iopub.status.busy":"2024-04-08T08:45:27.675474Z","iopub.status.idle":"2024-04-08T08:45:27.809847Z","shell.execute_reply":"2024-04-08T08:45:27.808702Z","shell.execute_reply.started":"2024-04-08T08:45:27.676515Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["###input Instruction:\n","谢娜是哪里的人啊？\n","###output: 她是四川的哦！\n"]}],"source":["print('###input',processed_datasets['train']['input'][8])\n","print('###output:',processed_datasets['train']['target'][8])"]},{"cell_type":"code","execution_count":142,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:45:28.891162Z","iopub.status.busy":"2024-04-08T08:45:28.890755Z","iopub.status.idle":"2024-04-08T08:45:28.897419Z","shell.execute_reply":"2024-04-08T08:45:28.896394Z","shell.execute_reply.started":"2024-04-08T08:45:28.891128Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'train': Dataset({\n","     features: ['input', 'target'],\n","     num_rows: 30225\n"," }),\n"," 'validation': Dataset({\n","     features: ['input', 'target'],\n","     num_rows: 4297\n"," }),\n"," 'test': Dataset({\n","     features: ['input', 'target'],\n","     num_rows: 5723\n"," })}"]},"execution_count":142,"metadata":{},"output_type":"execute_result"}],"source":["processed_datasets"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:58:29.435840Z","iopub.status.busy":"2024-04-08T09:58:29.435584Z","iopub.status.idle":"2024-04-08T10:00:03.080767Z","shell.execute_reply":"2024-04-08T10:00:03.079784Z","shell.execute_reply.started":"2024-04-08T09:58:29.435818Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fc5efd4dabc43778294f095f745bffa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/31 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b937497093dd44999a53473a9b13865e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f959ce7f67248688414ea000e6f3655","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def tokenize_function(examples):\n","    # Adjust \"max_length\" and \"padding\" according to your needs\n","    model_inputs = tokenizer(examples['input'], padding=\"max_length\", truncation=True, max_length=512)\n","    # Tokenize targets, ensure to set padding and truncation to match your model's requirements\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(examples['target'], padding=\"max_length\", truncation=True, max_length=128)\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_datasets = {split: processed_datasets[split].map(tokenize_function, batched=True) for split in processed_datasets.keys()}"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T08:51:50.049056Z","iopub.status.busy":"2024-04-08T08:51:50.048775Z","iopub.status.idle":"2024-04-08T08:51:50.055718Z","shell.execute_reply":"2024-04-08T08:51:50.054798Z","shell.execute_reply.started":"2024-04-08T08:51:50.049032Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'train': Dataset({\n","     features: ['input', 'target', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","     num_rows: 30225\n"," }),\n"," 'validation': Dataset({\n","     features: ['input', 'target', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","     num_rows: 4297\n"," }),\n"," 'test': Dataset({\n","     features: ['input', 'target', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n","     num_rows: 5723\n"," })}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets"]},{"cell_type":"markdown","metadata":{},"source":["**Load Model**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T10:00:03.082173Z","iopub.status.busy":"2024-04-08T10:00:03.081879Z","iopub.status.idle":"2024-04-08T10:00:06.470797Z","shell.execute_reply":"2024-04-08T10:00:06.470041Z","shell.execute_reply.started":"2024-04-08T10:00:03.082150Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1feb6ad70eb4016bb00994f2935b53c","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/561M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import BartForConditionalGeneration\n","\n","device=\"cuda:0\"\n","\n","model = BartForConditionalGeneration.from_pretrained(\n","    model_name,\n","    device_map=device,\n","    #quantization_config=quantization_config,\n","    low_cpu_mem_usage=True,\n","    #trust_remote_code=True\n",")\n","model.half()\n","model.config.use_cache = False"]},{"cell_type":"markdown","metadata":{},"source":["**Training Parameters**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T10:00:06.473369Z","iopub.status.busy":"2024-04-08T10:00:06.473081Z","iopub.status.idle":"2024-04-08T10:00:18.337826Z","shell.execute_reply":"2024-04-08T10:00:18.336886Z","shell.execute_reply.started":"2024-04-08T10:00:06.473345Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-08 10:00:09.776756: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-08 10:00:09.776870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-08 10:00:09.948376: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T10:00:18.340291Z","iopub.status.busy":"2024-04-08T10:00:18.339263Z","iopub.status.idle":"2024-04-08T10:00:18.417814Z","shell.execute_reply":"2024-04-08T10:00:18.416733Z","shell.execute_reply.started":"2024-04-08T10:00:18.340254Z"},"trusted":true},"outputs":[],"source":["from peft import LoraConfig\n","\n","peft_params = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    #task_type=\"CAUSAL_LM\",\n","    task_type=\"SEQ_2_SEQ_LM\",\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T10:00:18.419806Z","iopub.status.busy":"2024-04-08T10:00:18.419399Z","iopub.status.idle":"2024-04-08T10:00:18.580449Z","shell.execute_reply":"2024-04-08T10:00:18.579475Z","shell.execute_reply.started":"2024-04-08T10:00:18.419781Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 3,538,944 || all params: 143,731,968 || trainable%: 2.462182943184915\n"]}],"source":["from peft import get_peft_model\n","\n","model = get_peft_model(model, peft_params)\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T10:00:18.582656Z","iopub.status.busy":"2024-04-08T10:00:18.581899Z","iopub.status.idle":"2024-04-08T10:00:18.620988Z","shell.execute_reply":"2024-04-08T10:00:18.620286Z","shell.execute_reply.started":"2024-04-08T10:00:18.582623Z"},"trusted":true},"outputs":[],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir='./results',\n","    evaluation_strategy=\"epoch\",\n","    num_train_epochs=3,\n","    # 5 -> Not Good\n","    # 3 -> Quite Good\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    gradient_accumulation_steps=1,\n","    optim='paged_adamw_8bit',\n","    save_steps=25,\n","    logging_steps=25,\n","    learning_rate=2e-3, #2e-3 > 2e-4 > 2e-2\n","    #1e-3 0.29 0.52\n","    #2e-3 0.3 0.47\n","    #3e-3 0.33 0.28 Winner But Response are strange\n","    #4e-3 0.36 1.18\n","    weight_decay=0.003,\n","    fp16=False,\n","    bf16=False,\n","    max_grad_norm=0.3,\n","    max_steps=-1,\n","    warmup_ratio=0.03,\n","    group_by_length=True,\n","    lr_scheduler_type='constant',\n","    report_to='tensorboard',\n","    predict_with_generate=True\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T10:00:18.622251Z","iopub.status.busy":"2024-04-08T10:00:18.621959Z","iopub.status.idle":"2024-04-08T10:00:18.947242Z","shell.execute_reply":"2024-04-08T10:00:18.946335Z","shell.execute_reply.started":"2024-04-08T10:00:18.622228Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T10:00:18.948821Z","iopub.status.busy":"2024-04-08T10:00:18.948474Z","iopub.status.idle":"2024-04-08T10:01:14.540544Z","shell.execute_reply":"2024-04-08T10:01:14.539224Z","shell.execute_reply.started":"2024-04-08T10:00:18.948789Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='26' max='1419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  26/1419 00:16 < 15:32, 1.49 it/s, Epoch 0.05/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"RuntimeError","evalue":"\n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.model.decoder.embed_tokens.weight', 'base_model.model.model.encoder.embed_tokens.weight', 'base_model.model.model.shared.weight', 'base_model.model.lm_head.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2029\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2423\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2420\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2423\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2499\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     staging_output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2499\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaging_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   2503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(staging_output_dir)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3016\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3016\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3083\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3081\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors:\n\u001b[0;32m-> 3083\u001b[0m     \u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3087\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    251\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    252\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/safetensors/torch.py:477\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    474\u001b[0m         failing\u001b[38;5;241m.\u001b[39mappend(names)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124m        A potential way to correctly save your model is to use `save_model`.\u001b[39m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124m        More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\u001b[39m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    486\u001b[0m     k: {\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    492\u001b[0m }\n","\u001b[0;31mRuntimeError\u001b[0m: \n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.model.decoder.embed_tokens.weight', 'base_model.model.model.encoder.embed_tokens.weight', 'base_model.model.model.shared.weight', 'base_model.model.lm_head.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:29:15.307349Z","iopub.status.busy":"2024-04-08T09:29:15.306959Z","iopub.status.idle":"2024-04-08T09:29:15.832680Z","shell.execute_reply":"2024-04-08T09:29:15.831502Z","shell.execute_reply.started":"2024-04-08T09:29:15.307318Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["范 冰 冰 的 身 高 是 170cm 。\n"]}],"source":["model = model.to(device)\n","model.eval()\n","\n","inputs = tokenizer(\"范冰冰多重啊？\", return_tensors=\"pt\")\n","\n","outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), max_new_tokens=50)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{},"source":["**Evaluation**"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:29:55.832196Z","iopub.status.busy":"2024-04-08T09:29:55.831678Z","iopub.status.idle":"2024-04-08T09:29:55.838661Z","shell.execute_reply":"2024-04-08T09:29:55.837722Z","shell.execute_reply.started":"2024-04-08T09:29:55.832155Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['conversation'],\n","        num_rows: 6618\n","    })\n","    validation: Dataset({\n","        features: ['conversation'],\n","        num_rows: 946\n","    })\n","    test: Dataset({\n","        features: ['conversation'],\n","        num_rows: 2626\n","    })\n","})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"markdown","metadata":{},"source":["**Save Model**"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:33:12.840169Z","iopub.status.busy":"2024-04-08T09:33:12.839433Z","iopub.status.idle":"2024-04-08T09:33:12.982263Z","shell.execute_reply":"2024-04-08T09:33:12.981410Z","shell.execute_reply.started":"2024-04-08T09:33:12.840137Z"},"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(\"new_model\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:43:25.695011Z","iopub.status.busy":"2024-04-08T09:43:25.694572Z","iopub.status.idle":"2024-04-08T09:43:25.737728Z","shell.execute_reply":"2024-04-08T09:43:25.736358Z","shell.execute_reply.started":"2024-04-08T09:43:25.694971Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_model2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model.save_pretrained(\"new_model2\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-08T09:30:07.207263Z","iopub.status.busy":"2024-04-08T09:30:07.206390Z","iopub.status.idle":"2024-04-08T09:30:07.256106Z","shell.execute_reply":"2024-04-08T09:30:07.255141Z","shell.execute_reply.started":"2024-04-08T09:30:07.207226Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('new_model2/tokenizer_config.json',\n"," 'new_model2/special_tokens_map.json',\n"," 'new_model2/vocab.txt',\n"," 'new_model2/added_tokens.json')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.save_pretrained(\"new_model\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4659852,"sourceId":7928487,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
