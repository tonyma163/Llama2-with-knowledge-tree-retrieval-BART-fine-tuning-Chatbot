{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8144823,"sourceType":"datasetVersion","datasetId":4816177}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T04:19:50.813231Z","iopub.execute_input":"2024-04-20T04:19:50.814280Z","iopub.status.idle":"2024-04-20T04:19:50.838103Z","shell.execute_reply.started":"2024-04-20T04:19:50.814232Z","shell.execute_reply":"2024-04-20T04:19:50.837041Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b88f3827b844ffa5fd074a669635b7"}},"metadata":{}}]},{"cell_type":"code","source":"!pip install ctransformers accelerate peft keybert jieba chinese-synonym-word","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:18:27.760903Z","iopub.execute_input":"2024-04-20T04:18:27.761289Z","iopub.status.idle":"2024-04-20T04:18:47.241121Z","shell.execute_reply.started":"2024-04-20T04:18:27.761256Z","shell.execute_reply":"2024-04-20T04:18:47.240033Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting ctransformers\n  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\nCollecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nCollecting keybert\n  Downloading keybert-0.8.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (0.42.1)\nCollecting chinese-synonym-word\n  Downloading chinese_synonym_word-0.1.6-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from ctransformers) (0.22.2)\nRequirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from ctransformers) (9.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nCollecting sentence-transformers>=0.3.8 (from keybert)\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scikit-learn>=0.22.2 in /opt/conda/lib/python3.10/site-packages (from keybert) (1.2.2)\nRequirement already satisfied: rich>=10.4.0 in /opt/conda/lib/python3.10/site-packages (from keybert) (13.7.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->ctransformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (2.17.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (9.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chinese_synonym_word-0.1.6-py3-none-any.whl (42.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: keybert\n  Building wheel for keybert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keybert: filename=keybert-0.8.4-py3-none-any.whl size=39199 sha256=19fca65275b0315870a32aa8a25b02319090baa3f246b81a4b2df1d4ff58e632\n  Stored in directory: /root/.cache/pip/wheels/97/ef/4c/6588bd7072b0cc04225b40e639b991e49ebd4e21fb81f0acee\nSuccessfully built keybert\nInstalling collected packages: chinese-synonym-word, ctransformers, sentence-transformers, peft, keybert\nSuccessfully installed chinese-synonym-word-0.1.6 ctransformers-0.2.27 keybert-0.8.4 peft-0.10.0 sentence-transformers-2.7.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Load Llama2 for RAG**","metadata":{}},{"cell_type":"code","source":"from ctransformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n# Model\nmodel_llama = AutoModelForCausalLM.from_pretrained(\n    \"SinpxAI/Llama2-Chinese-7B-Chat-GGUF\",\n    model_file=\"llama2-chinese-7b-chat.Q4_K_M.gguf\",\n    model_type=\"llama\",\n    gpu_layers=110,\n    hf=True\n)\n\n# Tokenizer\ntokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:19:56.067664Z","iopub.execute_input":"2024-04-20T04:19:56.068452Z","iopub.status.idle":"2024-04-20T04:19:58.864434Z","shell.execute_reply.started":"2024-04-20T04:19:56.068417Z","shell.execute_reply":"2024-04-20T04:19:58.863294Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edefd196e9f3444594fed8ba799680df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3f7098508044aebb4d636d5edac1396"}},"metadata":{}},{"name":"stderr","text":"ggml_cuda_set_main_device: using device 0 (Tesla T4) as main device\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c7437d3eab3450f9bf7e28c7aebf77c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5958829f231e495b8dcbf752d3c73c53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e465cd623d149898b83ebf4c961eb00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed9337d38c1b476fb38310ba8bdcb5c8"}},"metadata":{}}]},{"cell_type":"code","source":"# Pipeline for Llama2\npipe_llama = pipeline(task=\"text-generation\", model=model_llama, tokenizer=tokenizer_llama)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:19:59.747210Z","iopub.execute_input":"2024-04-20T04:19:59.747617Z","iopub.status.idle":"2024-04-20T04:19:59.753219Z","shell.execute_reply.started":"2024-04-20T04:19:59.747574Z","shell.execute_reply":"2024-04-20T04:19:59.751927Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Load Fine-tuned Bart Model & Tokenizer**","metadata":{}},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, AutoTokenizer, pipeline\nfrom peft import PeftModel\nimport torch\n\nbase_model = \"fnlp/bart-base-chinese\"\nnew_model = \"tonyma163/bart_v1\"\n\ndevice=\"cuda:0\"\n\nbase_model_reload = BartForConditionalGeneration.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=device,\n        #trust_remote_code=True,\n)\nbase_model_reload.half()\n\nmodel_bart = PeftModel.from_pretrained(base_model_reload, new_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:20:15.778072Z","iopub.execute_input":"2024-04-20T04:20:15.778765Z","iopub.status.idle":"2024-04-20T04:20:21.513393Z","shell.execute_reply.started":"2024-04-20T04:20:15.778704Z","shell.execute_reply":"2024-04-20T04:20:21.512547Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8825f18df771416ab50603826beedcd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/561M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2584d9047204802af16ca458f8fd176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9327fa510445619bd9bfc1e0b57ea9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/7.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f3fb3b28e364e5aaa6597a49f854744"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer_bart = BertTokenizer.from_pretrained(base_model, trust_remote_code=True)\n\ntokenizer_bart.pad_token = tokenizer_bart.eos_token\ntokenizer_bart.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:20:21.515042Z","iopub.execute_input":"2024-04-20T04:20:21.515362Z","iopub.status.idle":"2024-04-20T04:20:22.243767Z","shell.execute_reply.started":"2024-04-20T04:20:21.515336Z","shell.execute_reply":"2024-04-20T04:20:22.242680Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d06f0cb6ca6c465b8cc21c66ff437e3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b6884c7341498bb535712c144f3319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39f54727c4054fa481a860379e7e59ad"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Text2TextGenerationPipeline\n\npipe_bart = Text2TextGenerationPipeline(model=model_bart, tokenizer=tokenizer_bart)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:20:24.978260Z","iopub.execute_input":"2024-04-20T04:20:24.978661Z","iopub.status.idle":"2024-04-20T04:20:24.985276Z","shell.execute_reply.started":"2024-04-20T04:20:24.978627Z","shell.execute_reply":"2024-04-20T04:20:24.984150Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The model 'PeftModelForSeq2SeqLM' is not supported for . Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Load Knowledge Set**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport ast\n\nfile_path = \"/kaggle/input/nlp-knowledge-set/knowledge_set.txt\"\n\ndata = []\n\n# Open the file and parse each line from string to tuple\nwith open(file_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        if line.strip():  # Ensure the line is not empty\n            try:\n                # Convert string representation of tuple to actual tuple\n                tuple_data = ast.literal_eval(line.strip())\n                data.append(tuple_data)\n            except SyntaxError:\n                print(f\"Skipping malformed line: {line.strip()}\")\n\n# Load the data into a DataFrame\ndf = pd.DataFrame(data, columns=['Entity', 'Category', 'Answer'])","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:21:24.567908Z","iopub.execute_input":"2024-04-20T04:21:24.568290Z","iopub.status.idle":"2024-04-20T04:21:25.123518Z","shell.execute_reply.started":"2024-04-20T04:21:24.568265Z","shell.execute_reply":"2024-04-20T04:21:25.122654Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:21:27.088809Z","iopub.execute_input":"2024-04-20T04:21:27.089214Z","iopub.status.idle":"2024-04-20T04:21:27.110605Z","shell.execute_reply.started":"2024-04-20T04:21:27.089187Z","shell.execute_reply":"2024-04-20T04:21:27.109513Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"        Entity    Category                 Answer\n0           西宁  2018-11-14  阴,东风,最高气温:5℃,最低气温:-4℃\n1          何霄玲          喜好                    poi\n2   快乐大本营之快乐到家          评论   不好意思啊坡姐，我是你的路人黑，对不住了\n3  辣相见川菜（三水总店）         特色菜                    水煮鱼\n4         浮城大亨          评论                人生是一幕大剧","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>Category</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>西宁</td>\n      <td>2018-11-14</td>\n      <td>阴,东风,最高气温:5℃,最低气温:-4℃</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>何霄玲</td>\n      <td>喜好</td>\n      <td>poi</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>快乐大本营之快乐到家</td>\n      <td>评论</td>\n      <td>不好意思啊坡姐，我是你的路人黑，对不住了</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>辣相见川菜（三水总店）</td>\n      <td>特色菜</td>\n      <td>水煮鱼</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>浮城大亨</td>\n      <td>评论</td>\n      <td>人生是一幕大剧</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Knowledge Tree**","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\n\n# Assuming 'df' is your DataFrame containing the data\nG = nx.DiGraph()  # Directed graph can still function as a tree\n\n# Add nodes and edges based on the DataFrame\nfor index, row in df.iterrows():\n    entity_id = f\"Entity: {row['Entity']}\"\n    category_id = f\"Category: {row['Category']} ({row['Entity']})\"\n    answer_id = f\"Answer: {row['Answer']} ({row['Category']})\"\n\n    # Ensure that nodes for each level (entity, category) are unique per entity-category pair\n    if entity_id not in G:\n        G.add_node(entity_id, type='Entity', name=row['Entity'])\n    if category_id not in G:\n        G.add_node(category_id, type='Category', name=row['Category'])\n    \n    # Answers can be multiple per category, so they are always added\n    G.add_node(answer_id, type='Answer', content=row['Answer'])\n    \n    # Connect nodes hierarchically\n    G.add_edge(entity_id, category_id)\n    G.add_edge(category_id, answer_id)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:21:28.674154Z","iopub.execute_input":"2024-04-20T04:21:28.674517Z","iopub.status.idle":"2024-04-20T04:21:32.403539Z","shell.execute_reply.started":"2024-04-20T04:21:28.674487Z","shell.execute_reply":"2024-04-20T04:21:32.402673Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**Part of Speech Function**","metadata":{}},{"cell_type":"code","source":"def checkSentence(sentence):\n    separated_sentences = []\n    temp_sentence = \"\"\n    in_bracket = False\n\n    for char in sentence:\n        if char == \"『\" or char == \"《\":\n            in_bracket = True\n            if temp_sentence:\n                separated_sentences.append(temp_sentence)\n                temp_sentence = \"\"\n            temp_sentence += char\n        elif char == \"』\" or char == \"》\":\n            in_bracket = False\n            temp_sentence += char\n            separated_sentences.append(temp_sentence)\n            temp_sentence = \"\"\n        elif char == \"。\" and not in_bracket:\n            separated_sentences.append(temp_sentence)\n            temp_sentence = \"\"\n        else:\n            temp_sentence += char\n\n    if temp_sentence:\n        separated_sentences.append(temp_sentence)\n\n    return separated_sentences","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:21:41.827880Z","iopub.execute_input":"2024-04-20T04:21:41.828289Z","iopub.status.idle":"2024-04-20T04:21:41.836416Z","shell.execute_reply.started":"2024-04-20T04:21:41.828255Z","shell.execute_reply":"2024-04-20T04:21:41.835162Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import jieba.posseg as pseg\n\ndef extractPOS(sentences):\n\n    result = []\n    special_words = []\n\n    for sentence in sentences:\n        if '『' in sentence or '《' in sentence:\n            # Remove the brackets\n            sentence = sentence.replace('『', '@').replace('』', '@').replace('《', '@').replace('》', '@')\n            #result.append(sentence)\n            special_words.append(sentence)\n        else:\n            allowSentencePOS = (\"a\", \"ad\", \"ag\", \"an\", ## 形容词 \n                                \"b\", # 区别词\n                                \"c\", # 连词\n                                \"f\", \n                                \"g\", \n                                \"h\",\n                                \"i\", \n                                \"j\", \n                                \"k\", \n                                \"l\", \n                                \"m\", \"mg\", \"mq\", \n                                \"n\", \"ng\", \"nr\", \"nrfg\", \"nrt\",\"ns\", \"nt\", \"nz\", \n                                \"o\", \n                                \"q\", \n                                \"s\", \n                                \"t\", \"tg\",\n                                \"v\", \"vd\", \"vg\", \"vi\", \"vn\",\"vq\" ## 动词\n                                )\n\n            exclude = ( \"p\", ## 介词\n                \"d\", \"df\", \"dg\", # 副词\n                \"e\", # 叹词 \n                \"r\", \"rg\", \"rr\", \"rz\",  ## 代词\n                \"u\",  \"ud\", \"ug\", \"uj\", \"ul\", \"uv\", \"uz\",  ## 助词\n                \"x\",  ## 非语素词（包含标点符号\n                \"y\", ## 语气词\n                \"z\",\"zg\" ## 助词\n              )\n\n            sentence = pseg.lcut(sentence)\n            seg_words = [word for word, pos in sentence if pos in allowSentencePOS]\n            result.extend(seg_words)\n            \n    return special_words+result","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:21:44.101115Z","iopub.execute_input":"2024-04-20T04:21:44.102205Z","iopub.status.idle":"2024-04-20T04:21:44.970200Z","shell.execute_reply.started":"2024-04-20T04:21:44.102163Z","shell.execute_reply":"2024-04-20T04:21:44.969215Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Retrieve Data from Knowledge Tree**","metadata":{}},{"cell_type":"code","source":"def has_common_character(keyword, category):\n    return any(char in category for char in keyword)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:22.427249Z","iopub.execute_input":"2024-04-20T04:22:22.427690Z","iopub.status.idle":"2024-04-20T04:22:22.433011Z","shell.execute_reply.started":"2024-04-20T04:22:22.427657Z","shell.execute_reply":"2024-04-20T04:22:22.431687Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import jieba\n\ndef retrieve_answers(graph, query):\n    segmented_keywords = extractPOS(checkSentence(query))\n    special_keywords = [kw.strip('@') for kw in segmented_keywords if kw.startswith('@')]  # Process special keywords\n    regular_keywords = [kw for kw in segmented_keywords if not kw.startswith('@')]  # Regular keywords\n\n    found_answers = []\n\n    # Top-Down Search: From entities to categories to answers with exact matching for entities\n    for entity_node in (n for n in graph.nodes if graph.nodes[n]['type'] == 'Entity'):\n        # Exact match required for entity names\n        entity_keywords = [kw for kw in regular_keywords if kw == graph.nodes[entity_node]['name']]\n        if entity_keywords:\n            for category_node in graph.successors(entity_node):\n                category_keywords = [kw for kw in regular_keywords if has_common_character(kw, graph.nodes[category_node]['name'])]\n                if category_keywords:\n                    for answer_node in graph.successors(category_node):\n                        found_answers.append({\n                            #graph.nodes[entity_node]['name']+\n                            graph.nodes[category_node]['name']+\n                            graph.nodes[answer_node]['content']\n                        })\n                        \"\"\"found_answers.append({\n                            'entity': graph.nodes[entity_node]['name'],\n                            'category': graph.nodes[category_node]['name'],\n                            'answer': graph.nodes[answer_node]['content']\n                        })\"\"\"\n\n    # Bottom-Up Search: From answers to categories to entities, but only using special keywords\n    for answer_node in (n for n in graph.nodes if graph.nodes[n].get('type') == 'Answer'):\n        if any(kw == graph.nodes[answer_node]['content'] for kw in special_keywords):  # Exact match check for special keywords\n            for category_node in graph.predecessors(answer_node):\n                if any(has_common_character(kw, graph.nodes[category_node]['name']) for kw in regular_keywords):\n                    for entity_node in graph.predecessors(category_node):\n                        found_answers.append({\n                            graph.nodes[entity_node]['name']+\n                            graph.nodes[category_node]['name']\n                            #graph.nodes[answer_node]['content']\n                        })\n                        \"\"\"found_answers.append({\n                            'entity': graph.nodes[entity_node]['name'],\n                            'category': graph.nodes[category_node]['name'],\n                            'answer': graph.nodes[answer_node]['content']\n                        })\"\"\"\n\n    return found_answers","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:23.719630Z","iopub.execute_input":"2024-04-20T04:22:23.720763Z","iopub.status.idle":"2024-04-20T04:22:23.733193Z","shell.execute_reply.started":"2024-04-20T04:22:23.720706Z","shell.execute_reply":"2024-04-20T04:22:23.732141Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Q1\ninput_query = \"你知道张国荣的星座吗？\"\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")\n\n#Q2\ninput_query = \"周迅的星座是什么?\"\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")\n\n#Q3\ninput_query = \"范冰冰多重啊？\"\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")\n\n#Q4\ninput_query = \"『爱你等于爱自己』这首歌的主唱是谁啊？\"\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")\n\n#Q5\ninput_query = \"电影《倾城》主演是谁?\"\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")\n\n#Q6\ninput_query = \"说说张学友的新闻\"\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:28.804675Z","iopub.execute_input":"2024-04-20T04:22:28.805079Z","iopub.status.idle":"2024-04-20T04:22:29.265918Z","shell.execute_reply.started":"2024-04-20T04:22:28.805046Z","shell.execute_reply":"2024-04-20T04:22:29.264935Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Question: 你知道张国荣的星座吗？ Response: [{'星座处女座'}]\nQuestion: 周迅的星座是什么? Response: [{'星座天秤座'}]\nQuestion: 范冰冰多重啊？ Response: [{'体重56kg'}]\nQuestion: 『爱你等于爱自己』这首歌的主唱是谁啊？ Response: [{'王力宏演唱'}]\nQuestion: 电影《倾城》主演是谁? Response: [{'林心如主演'}]\nQuestion: 说说张学友的新闻 Response: [{'新闻近日，江苏苏州，12月28日至30日张学友苏州演唱会再次上演“神助功”，三天时间里，苏州公安在全市范围内共抓获22名在逃人员。此前，苏州公安曾转发张学友演唱会苏州站预告，并称“懂了，懂了”。网友:公安战线著名歌手张学友要准备冲年度KPI了？'}, {'新闻距1/2世纪巡回演唱会已过去4年，今年，歌神再度回归，张学友2016,世界巡回演唱会北京站——“卡萨帝之夜”（张学友经典演唱会北京联合赞助）将于乐视体育中心开幕，呈现一场音乐与艺术的经典盛筵。那些年我们追过的歌神演唱会'}, {'新闻据四川遂宁警方介绍，21日晚张学友在遂宁第一场演唱会，警方挡获扒窃、卖假票、冒充工作人员骗钱骗票等涉嫌违法犯罪人员10余名。警方提醒:享受音乐盛宴的同时，也要提高警惕、谨防扒窃及各种骗局。（成都商报）?张学友演唱会又抓嫌犯！遂宁开唱第一场，警方就挡获10多人'}, {'新闻9月30日，陕西咸阳\"逃犯克星\"张学友举行演唱会时再次\"立功\"咸阳公安通报，不仅有5名逃犯落网，还抓获了其它违法犯罪人员13人9月28日晚，张学友演唱会在石家庄举行时便成功抓获了3名逃犯向娱乐圈卧底探员张Sir致敬！“逃犯克星”张学友又立功5名逃犯在咸阳落网'}, {'新闻据平安洛阳7月8日，民警在张学友演唱会现场附近执勤时发现一车辆交通违法，驾车男子提供个人信息不实，身份可疑。原来该男子从563公里远的武汉赶到洛阳，冒用他人信息无证驾驶，就是为看张学友演唱会。警察蜀黍棒棒哒(人民网)逃犯克星张学友上演第七杀？洛阳演唱会前一男子被抓'}, {'新闻7月8日，河南洛阳民警在张学友演唱会现场附近执勤时，发现一车辆存在交通违法，遂上前了解情况。经查，驾车男子提供的个人信息与其本人不符，身份可疑。经进一步核实，原来该男子从563公里远的武汉赶到洛阳，就是为看张学友演唱会。目前，该男子对冒用他人信息，无证驾驶行为供认不讳，案件正在进一步调查中。据悉，洛阳演唱会现场蜀黍还抓了一波黄牛党，正在核实中。'}, {'新闻22日晚，广东江门市体育中心张学友演唱会，在安检通道检票时，民警通过人像识别系统成功识别出一名在逃人员范某并抓获。据粗略统计，自今年4月第一起，警方已在张学友演唱会上陆续抓捕了约60名犯罪嫌疑人或在逃人员。'}, {'新闻据四川遂宁警方介绍，21日晚张学友在遂宁第一场演唱会，警方挡获扒窃、卖假票、冒充工作人员骗钱骗票等涉嫌违法犯罪人员10余名。警方提醒:享受音乐盛宴的同时，也要提高警惕、谨防扒窃及各种骗局。（成都商报）\\u200b张学友演唱会又抓嫌犯！遂宁开唱第一场，警方就挡获10多人'}, {'新闻【张学友演唱会又双叒叕抓到嫌犯，四川遂宁开唱第一场抓了十余人】据四川遂宁警方介绍，21日晚张学友在遂宁第一场演唱会，警方挡获扒窃、卖假票、冒充工作人员骗钱骗票等涉嫌违法犯罪人员10余名。警方提醒：享受音乐盛宴的同时，也要提高警惕、谨防扒窃及各种骗局。\\u200b张学友演唱会又抓嫌犯！遂宁开唱第一场，警方就挡获10多人'}, {'新闻9月30日，陕西咸阳，“逃犯克星”张学友举行演唱会时再次“立功”。咸阳公安通报，5名逃犯落网，抓获了其它违法犯罪人员13人。9月28日晚，张学友演唱会在河北石家庄举行，成功抓获了3名逃犯。'}]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Llama2(Knowledge Tree) + Fine-tuned Bart**","metadata":{}},{"cell_type":"code","source":"system_prompt = \"\"\"\nPlease answer the following question based on the provided context.\nProvide only the direct answers without any additional explanations or context.\nPlease only output answers only.\nPlease ensure that your responses are socially unbiased and positive in nature.\nIf a question does not make any sense or is not factually coherent, explain why instead of answering something not correct.\nIf you don't know the answer, please don't share false information.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:33.239944Z","iopub.execute_input":"2024-04-20T04:22:33.240431Z","iopub.status.idle":"2024-04-20T04:22:33.246512Z","shell.execute_reply.started":"2024-04-20T04:22:33.240390Z","shell.execute_reply":"2024-04-20T04:22:33.245206Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def query_llama(query, context):\n    # Test\n    prompt = f\"\"\"\n    <s>[INST] <<SYS>>\n    {system_prompt}\n     <</SYS>> [/INST]</s>\n\n    <s>[INST]\n    Question: {query}\n    Context: {context}\n    Answer:\n    [/INST]\n        \"\"\"\n\n    output = pipe_llama(\n        prompt,\n        do_sample=True,\n        max_new_tokens=256,\n        top_k=40,\n        top_p=0.95,\n        temperature=0.75,\n        return_full_text=False,\n        repetition_penalty=1\n    )\n    return output[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:34.654235Z","iopub.execute_input":"2024-04-20T04:22:34.654923Z","iopub.status.idle":"2024-04-20T04:22:34.660772Z","shell.execute_reply.started":"2024-04-20T04:22:34.654872Z","shell.execute_reply":"2024-04-20T04:22:34.659604Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def query_system(graph, query):\n    # Attempt to retrieve answers from the knowledge graph\n    answers = retrieve_answers(graph, query)\n    \n    # If answers are found in the graph, return\n    if answers:\n        # pass to llama\n        return \"Answer from knowledge graph:\", query_llama(query, answers)\n    \n    # If no answers are found, -> language model\n    else:\n        generated_answer = pipe_bart(query)\n        return \"Answer from language model:\", generated_answer[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:36.110021Z","iopub.execute_input":"2024-04-20T04:22:36.110416Z","iopub.status.idle":"2024-04-20T04:22:36.117311Z","shell.execute_reply.started":"2024-04-20T04:22:36.110384Z","shell.execute_reply":"2024-04-20T04:22:36.116142Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**Test**","metadata":{}},{"cell_type":"code","source":"query = \"周迅的星座是什么？\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:40.726255Z","iopub.execute_input":"2024-04-20T04:22:40.727110Z","iopub.status.idle":"2024-04-20T04:22:47.321219Z","shell.execute_reply.started":"2024-04-20T04:22:40.727068Z","shell.execute_reply":"2024-04-20T04:22:47.320069Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"('Answer from knowledge graph:', '周迅的星座是天秤座。')\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"你知道张柏芝的生日是什么时候吗？\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:47.323044Z","iopub.execute_input":"2024-04-20T04:22:47.323385Z","iopub.status.idle":"2024-04-20T04:22:50.539056Z","shell.execute_reply.started":"2024-04-20T04:22:47.323354Z","shell.execute_reply":"2024-04-20T04:22:50.537994Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"('Answer from knowledge graph:', '生日：1980-5-24\\n')\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"『爱你等于爱自己』这首歌的主唱是谁啊？\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:50.540035Z","iopub.execute_input":"2024-04-20T04:22:50.540317Z","iopub.status.idle":"2024-04-20T04:22:52.932598Z","shell.execute_reply.started":"2024-04-20T04:22:50.540291Z","shell.execute_reply":"2024-04-20T04:22:52.931596Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"('Answer from knowledge graph:', '王力宏')\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"电影《倾城》主演是谁?\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:52.934712Z","iopub.execute_input":"2024-04-20T04:22:52.935044Z","iopub.status.idle":"2024-04-20T04:22:56.083449Z","shell.execute_reply.started":"2024-04-20T04:22:52.935017Z","shell.execute_reply":"2024-04-20T04:22:56.082422Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"('Answer from knowledge graph:', '林心如主演电影《倾城》。')\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"帮我点一首《想你》吧\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:56.084799Z","iopub.execute_input":"2024-04-20T04:22:56.085095Z","iopub.status.idle":"2024-04-20T04:22:57.321402Z","shell.execute_reply.started":"2024-04-20T04:22:56.085068Z","shell.execute_reply":"2024-04-20T04:22:57.320397Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"('Answer from language model:', '正 在 为 你 播 放 《 想 你 》')\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"张国荣哪里出生的？\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:22:57.322880Z","iopub.execute_input":"2024-04-20T04:22:57.323281Z","iopub.status.idle":"2024-04-20T04:23:00.100777Z","shell.execute_reply.started":"2024-04-20T04:22:57.323244Z","shell.execute_reply":"2024-04-20T04:23:00.099593Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"('Answer from knowledge graph:', '张国荣出生于香港九龙。\\n')\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"今天是什么日子了？\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:23:00.102170Z","iopub.execute_input":"2024-04-20T04:23:00.102576Z","iopub.status.idle":"2024-04-20T04:23:00.321792Z","shell.execute_reply.started":"2024-04-20T04:23:00.102524Z","shell.execute_reply":"2024-04-20T04:23:00.320829Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"('Answer from language model:', '今 天 是 2018 年 10 月 18 日')\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"说说张学友的新闻\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T04:23:00.322993Z","iopub.execute_input":"2024-04-20T04:23:00.323311Z","iopub.status.idle":"2024-04-20T04:24:36.605364Z","shell.execute_reply.started":"2024-04-20T04:23:00.323283Z","shell.execute_reply":"2024-04-20T04:24:36.604290Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"('Answer from knowledge graph:', \"\\n    I don'这三个人民仴公安公安局长期��人��不提什苬告��。\\n    I don'歌神助力控制定位于  * 张学友公安公安局面对于近日������������������������������������������������局长期��，] \\n    I cannot provide only the news near  No direct answer: \\n    Based on the newstories. 张学友网友网友消息，根据四川��了解����������通过去年����不知道��州公安局部分����������������������������到������������������所以上面对于22222222\")\n","output_type":"stream"}]}]}