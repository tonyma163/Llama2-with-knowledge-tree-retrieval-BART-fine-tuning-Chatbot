{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8144823,"sourceType":"datasetVersion","datasetId":4816177}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain accelerate peft keybert jieba","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T02:10:03.548949Z","iopub.execute_input":"2024-04-18T02:10:03.549994Z","iopub.status.idle":"2024-04-18T02:10:29.550286Z","shell.execute_reply.started":"2024-04-18T02:10:03.549959Z","shell.execute_reply":"2024-04-18T02:10:29.548931Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nCollecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nCollecting keybert\n  Downloading keybert-0.8.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (0.42.1)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.32 (from langchain)\n  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\nCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.48-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nCollecting sentence-transformers>=0.3.8 (from keybert)\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: scikit-learn>=0.22.2 in /opt/conda/lib/python3.10/site-packages (from keybert) (1.2.2)\nRequirement already satisfied: rich>=10.4.0 in /opt/conda/lib/python3.10/site-packages (from keybert) (13.7.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nCollecting packaging>=20.0 (from accelerate)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.4.0->keybert) (2.17.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2->keybert) (3.2.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.3.8->keybert) (9.5.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading langchain-0.1.16-py3-none-any.whl (817 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.44-py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.48-py3-none-any.whl (113 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: keybert\n  Building wheel for keybert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keybert: filename=keybert-0.8.4-py3-none-any.whl size=39199 sha256=cfaef281fb881ff1cf0839d71750ff1349650d7356698894e1c29e7498d3ea63\n  Stored in directory: /root/.cache/pip/wheels/97/ef/4c/6588bd7072b0cc04225b40e639b991e49ebd4e21fb81f0acee\nSuccessfully built keybert\nInstalling collected packages: packaging, orjson, langsmith, langchain-core, sentence-transformers, peft, langchain-text-splitters, langchain-community, langchain, keybert\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keybert-0.8.4 langchain-0.1.16 langchain-community-0.0.33 langchain-core-0.1.44 langchain-text-splitters-0.0.1 langsmith-0.1.48 orjson-3.10.1 packaging-23.2 peft-0.10.0 sentence-transformers-2.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BartForConditionalGeneration\nfrom peft import PeftModel\nimport torch\n\nbase_model = \"fnlp/bart-base-chinese\"\nnew_model = \"tonyma163/bart_v1\"\n\ndevice=\"cuda:0\"\n\nbase_model_reload = BartForConditionalGeneration.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=device,\n        #trust_remote_code=True,\n)\nbase_model_reload.half()\n\nmodel = PeftModel.from_pretrained(base_model_reload, new_model)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:54:14.805630Z","iopub.execute_input":"2024-04-18T02:54:14.806088Z","iopub.status.idle":"2024-04-18T02:54:19.693018Z","shell.execute_reply.started":"2024-04-18T02:54:14.806057Z","shell.execute_reply":"2024-04-18T02:54:19.691673Z"},"trusted":true},"execution_count":109,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.69k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52acb23d0be74fb7a1892426a22540c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/561M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067242fae4194b8aade5bd8e608f1696"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c3b4acafef8416e826f312957aad008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/7.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"755296cf5d0448a28de3832a0bb806b1"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(base_model, trust_remote_code=True)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:54:19.695307Z","iopub.execute_input":"2024-04-18T02:54:19.695733Z","iopub.status.idle":"2024-04-18T02:54:20.638621Z","shell.execute_reply.started":"2024-04-18T02:54:19.695696Z","shell.execute_reply":"2024-04-18T02:54:20.637565Z"},"trusted":true},"execution_count":110,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/479 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c7985fb9c1549f7bc8f53c3a9bfc945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/259k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca9585d0d0f746a1a2d36464ceaf3b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f3daba90d984322bc2f093f9670a034"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import Text2TextGenerationPipeline\n\npipe = Text2TextGenerationPipeline(model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:54:20.639969Z","iopub.execute_input":"2024-04-18T02:54:20.640299Z","iopub.status.idle":"2024-04-18T02:54:20.646997Z","shell.execute_reply.started":"2024-04-18T02:54:20.640271Z","shell.execute_reply":"2024-04-18T02:54:20.645896Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stderr","text":"The model 'PeftModelForSeq2SeqLM' is not supported for . Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Loading Document**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport ast\n\nfile_path = \"/kaggle/input/nlp-knowledge-set/knowledge_set.txt\"\n\ndata = []\n\n# Open the file and parse each line from string to tuple\nwith open(file_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        if line.strip():  # Ensure the line is not empty\n            try:\n                # Convert string representation of tuple to actual tuple\n                tuple_data = ast.literal_eval(line.strip())\n                data.append(tuple_data)\n            except SyntaxError:\n                print(f\"Skipping malformed line: {line.strip()}\")\n\n# Load the data into a DataFrame\ndf = pd.DataFrame(data, columns=['Entity', 'Category', 'Answer'])","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:19:18.437777Z","iopub.execute_input":"2024-04-18T02:19:18.438750Z","iopub.status.idle":"2024-04-18T02:19:19.005691Z","shell.execute_reply.started":"2024-04-18T02:19:18.438706Z","shell.execute_reply":"2024-04-18T02:19:19.004751Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:19:28.092733Z","iopub.execute_input":"2024-04-18T02:19:28.093165Z","iopub.status.idle":"2024-04-18T02:19:28.110400Z","shell.execute_reply.started":"2024-04-18T02:19:28.093136Z","shell.execute_reply":"2024-04-18T02:19:28.109346Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"        Entity    Category                 Answer\n0           西宁  2018-11-14  阴,东风,最高气温:5℃,最低气温:-4℃\n1          何霄玲          喜好                    poi\n2   快乐大本营之快乐到家          评论   不好意思啊坡姐，我是你的路人黑，对不住了\n3  辣相见川菜（三水总店）         特色菜                    水煮鱼\n4         浮城大亨          评论                人生是一幕大剧","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>Category</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>西宁</td>\n      <td>2018-11-14</td>\n      <td>阴,东风,最高气温:5℃,最低气温:-4℃</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>何霄玲</td>\n      <td>喜好</td>\n      <td>poi</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>快乐大本营之快乐到家</td>\n      <td>评论</td>\n      <td>不好意思啊坡姐，我是你的路人黑，对不住了</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>辣相见川菜（三水总店）</td>\n      <td>特色菜</td>\n      <td>水煮鱼</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>浮城大亨</td>\n      <td>评论</td>\n      <td>人生是一幕大剧</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Knowledge Graph**","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add nodes and edges based on the DataFrame\nfor index, row in df.iterrows():\n    entity_node = f\"Entity: {row['Entity']}\"\n    category_node = f\"{entity_node} - Category: {row['Category']}\"\n    answer_node = f\"{category_node} - Answer: {row['Answer']}\"\n\n    # Add nodes and edges\n    G.add_node(entity_node, type='Entity')\n    G.add_node(category_node, type='Category')\n    G.add_node(answer_node, type='Answer', answer=row['Answer'])\n    \n    G.add_edge(entity_node, category_node)\n    G.add_edge(category_node, answer_node)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:40:45.278521Z","iopub.execute_input":"2024-04-18T02:40:45.279007Z","iopub.status.idle":"2024-04-18T02:40:47.928730Z","shell.execute_reply.started":"2024-04-18T02:40:45.278973Z","shell.execute_reply":"2024-04-18T02:40:47.927585Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# Example query: Find all answers linked to a specific entity and category\nentity_query = '西宁'\ncategory_query = '2018-11-14'\n\nprocessed_entity_query = f\"Entity: {entity_query}\"\nprocessed_category_query = f\"{processed_entity_query} - Category: {category_query}\"\n\n# First, find the category node directly connected to the entity\nif (processed_entity_query, processed_category_query) in G.edges:\n    answers = [node for node in G.successors(processed_category_query) if G.nodes[node]['type'] == 'Answer']\n    for answer in answers:\n        print(answer)\nelse:\n    print(\"No such category for the given entity or wrong category/entity combination.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:40:48.327024Z","iopub.execute_input":"2024-04-18T02:40:48.327413Z","iopub.status.idle":"2024-04-18T02:40:48.335283Z","shell.execute_reply.started":"2024-04-18T02:40:48.327386Z","shell.execute_reply":"2024-04-18T02:40:48.334283Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Entity: 西宁 - Category: 2018-11-14 - Answer: 阴,东风,最高气温:5℃,最低气温:-4℃\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**KeyBert + Chinese Word Segmentation**","metadata":{}},{"cell_type":"code","source":"from keybert import KeyBERT\n#周迅 的 星座 是 什么 ?\n#你好，今天是几号了？\n#prompt = \"周迅 的 星座 是 什么 ?\"\n\nkw_model = KeyBERT()\n#keywords = kw_model.extract_keywords(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:11:50.990133Z","iopub.execute_input":"2024-04-18T02:11:50.990512Z","iopub.status.idle":"2024-04-18T02:11:51.655607Z","shell.execute_reply.started":"2024-04-18T02:11:50.990486Z","shell.execute_reply":"2024-04-18T02:11:51.654700Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import jieba\n\nprompt = \"周迅的星座是什么?\"\n\n# Chinese Word Segmentation\nprocessed_prompt = jieba.lcut(prompt)\nprocessed_prompt = ' '.join(processed_prompt)\n\ntest_keywords_with_scores = kw_model.extract_keywords(processed_prompt, keyphrase_ngram_range=(1, 1), stop_words=None)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:51:06.830332Z","iopub.execute_input":"2024-04-18T02:51:06.830762Z","iopub.status.idle":"2024-04-18T02:51:06.863510Z","shell.execute_reply.started":"2024-04-18T02:51:06.830728Z","shell.execute_reply":"2024-04-18T02:51:06.862542Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# Extract only the keywords, discarding the scores\ntest_keywords = [keyword for keyword, _ in test_keywords_with_scores]\ntest_keywords","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:51:07.591803Z","iopub.execute_input":"2024-04-18T02:51:07.592217Z","iopub.status.idle":"2024-04-18T02:51:07.601308Z","shell.execute_reply.started":"2024-04-18T02:51:07.592187Z","shell.execute_reply":"2024-04-18T02:51:07.600258Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"['星座', '周迅', '什么']"},"metadata":{}}]},{"cell_type":"markdown","source":"**RAG**","metadata":{}},{"cell_type":"code","source":"# Chinese Word Segmentation\ndef seg_keywords(query):\n    processed_query = jieba.lcut(query)\n    processed_query = ' '.join(processed_query)\n\n    keywords_with_scores = kw_model.extract_keywords(processed_query, keyphrase_ngram_range=(1, 1), stop_words=None)\n    \n    # Extract only the keywords, discarding the scores\n    keywords = [keyword for keyword, _ in keywords_with_scores]\n    return keywords","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:47:08.327061Z","iopub.execute_input":"2024-04-18T02:47:08.328261Z","iopub.status.idle":"2024-04-18T02:47:08.334078Z","shell.execute_reply.started":"2024-04-18T02:47:08.328225Z","shell.execute_reply":"2024-04-18T02:47:08.332900Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"def retrieve_answers(graph, query):\n    keywords = seg_keywords(query)\n    \n    found_answers = []\n    # Traverse the graph looking for matches\n    for entity_node in (n for n in graph.nodes if graph.nodes[n].get('type') == 'Entity'):\n        entity_keywords = [kw for kw in keywords if kw in entity_node]\n        if entity_keywords:\n            # Explore each category node linked to the entity\n            for category_node in graph.successors(entity_node):\n                category_keywords = [kw for kw in keywords if kw in category_node and kw not in entity_keywords]\n                if category_keywords:\n                    # Collect all answers under each matching category\n                    answer_nodes = [node for node in graph.successors(category_node) if graph.nodes[node]['type'] == 'Answer']\n                    for answer_node in answer_nodes:\n                        found_answers.append(graph.nodes[answer_node]['answer'])  # Use 'answer' attribute\n\n    return found_answers\n","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:47:09.231074Z","iopub.execute_input":"2024-04-18T02:47:09.231818Z","iopub.status.idle":"2024-04-18T02:47:09.240048Z","shell.execute_reply.started":"2024-04-18T02:47:09.231787Z","shell.execute_reply":"2024-04-18T02:47:09.239009Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"#周迅的星座是什么?\n#你知道张国荣的星座吗？\ninput_query = \"你知道张国荣的星座吗？\"\n\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")\n\ninput_query = \"周迅的星座是什么?\"\n\nanswers = retrieve_answers(G, input_query)\nprint(f\"Question: {input_query} Response: {answers}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:49:51.321129Z","iopub.execute_input":"2024-04-18T02:49:51.321559Z","iopub.status.idle":"2024-04-18T02:49:51.449846Z","shell.execute_reply.started":"2024-04-18T02:49:51.321531Z","shell.execute_reply":"2024-04-18T02:49:51.448693Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"Question: 你知道张国荣的星座吗？ Response: ['处女座']\nQuestion: 周迅的星座是什么? Response: ['天秤座']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**RAG + LLM**","metadata":{}},{"cell_type":"code","source":"def query_system(graph, query):\n    # Attempt to retrieve answers from the knowledge graph\n    answers = retrieve_answers(graph, query)\n    \n    # If answers are found in the graph, return them\n    if answers:\n        return \"Answer from knowledge graph:\", answers\n    \n    # If no answers are found, defer to the language model\n    else:\n        generated_answer = pipe(query)\n        return \"Answer from language model:\", generated_answer[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:54:37.149602Z","iopub.execute_input":"2024-04-18T02:54:37.150392Z","iopub.status.idle":"2024-04-18T02:54:37.156620Z","shell.execute_reply.started":"2024-04-18T02:54:37.150358Z","shell.execute_reply":"2024-04-18T02:54:37.155360Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"query = \"周迅的星座是什么？\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:54:38.321166Z","iopub.execute_input":"2024-04-18T02:54:38.321999Z","iopub.status.idle":"2024-04-18T02:54:38.389475Z","shell.execute_reply.started":"2024-04-18T02:54:38.321968Z","shell.execute_reply":"2024-04-18T02:54:38.388441Z"},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"('Answer from knowledge graph:', ['天秤座'])\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"亲爱的，你知道《思念的距离》的主唱是谁吗\"\nresult = query_system(G, query)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T02:55:11.537270Z","iopub.execute_input":"2024-04-18T02:55:11.537701Z","iopub.status.idle":"2024-04-18T02:55:12.210646Z","shell.execute_reply.started":"2024-04-18T02:55:11.537670Z","shell.execute_reply":"2024-04-18T02:55:12.209623Z"},"trusted":true},"execution_count":116,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"('Answer from language model:', '这 首 歌 的 主 唱 是 周 杰 伦 哦 。')\n","output_type":"stream"}]}]}